### Building knowledge vectors from Wiki dumps

The following code can be used to build knowledge vectors from wikidumps. 

In order to do this you will need to download a dump. This is an easy enough thing to source online. I used [this site (May, 2024)](https://wikimedia.bringyour.com/enwiki/20240501/), but there are many mirrors to choose from. A simple Google search should get you where you're going. From here there a bunch of choices of things to download. In the context of this I used the main pages as a single dump `enwiktionary-latest-pages-articles.xml`. However, some downloads represent slices of the whole, and others that represent aspects of pages--and this would certainly be worth considering in the future for a more surgical approach.  The main dump can also be easily parsed using `import lxml.etree as etree`. The advantage is that it doesn't have to be loaded all into memory at once. The disadvantage is that it is not as easily multiprocessed, and it isn't indexed. 

### Steps

* #### Identify page titles
    * This is the easiest and can be done a few ways. The simplest is to download `enwiki-20240501-all-titles`, which is a txt file of all the pages. Alternatively a single pass through the `enwiktionary-latest-pages-articles.xml` where you grab the title of every page will give you the same thing. In either case you might want to do some filtering. ***!CAREFUL!*** certain forms of preprocessing (such as .lower()) could have unintended consequences. Within the dump some pages are essentially redirect pages, so 'bill clinton' might in fact be a page, but it is not the same page as 'Bill Clinton', one is generally a redirect to another, and thus contains a link to the other, but nothing else.
    *  One form of filtering is to remove pages that have little or no links. To do this you can use `filter_wikipedia.py`, running this script will produce a new filtered dump which removed pages with 1 or fewer links. 
* #### Identify verbs of interest
    * I had previously scraped wiktionary and built a verb framework. Like all the other things, I could go back and do better, but it gave me a fairly large pool of verbs to use. This is in the data folder as `verb_tree`. For our purposes we are going to want to turn this into a straight list, for which you can used `flatten_verb_tree.py`. You can use that list to get verb counts from wikipedia pages using `wiki_verb_impact.py`. 
* #### Get links and link impact
    * You can then use `find_links.py`, which creates a mapping of titles to links (i.e. what are the links on a particular page), and then use `find_link_impact.py` in order to see which links are most impactful. With this we can do some filtering, note the filtering scripts such as: `filter_links.py`. 
* #### Map links and verbs to embeddings
    * From here we can build an embedding space using `build_semantic.py`. This creates a pair of embedding docs: `link_embeddings.json` and `verb_embeddings.json`, one for verbs and one for links. We want the pair because we can potentially use both embedding spaces (either in tandem or separately), and also because we are going to get the intersected links and verbs, to get phrases. 
* #### Main mining algorithm through Wikipages
    * Now we're ready to use the `mine_embeddings.py` code. This works with multiprocessing to speed up, and in that way it is relatively fast to process a full wiki dump. Here I have an example produced on a single slice of the wikidump. `enwiki-20240501-pages-articles15.xml-p17324603p17460152`. The result of the code should be two files: `intersected_sentences.jsonl` and `page_embeddings.jsonl`. It might be worth considering put it straight into a db, but since this is just an intermediate processing point it is perhaps most efficient to keep it in json or jsonl. At any point, depending on your computational resources you can run `jsonltojson.py` which can extract the verb embeddings, the link embeddings (or both) and save them to a .json file structure. Here you'll see see `linkout.json`. 
* #### Shrink links to fit a 10k semantic space
    * Assuming you ran this on a full wiki dump you would then have every title with all of its page links. A full wikidump (as of this writing) is around 20 million pages. Some of those are vacuous, but even with those removed we're north of 15 million pages. If the goal is to get a page to know not only what it points to but also what is similar to it, we need to be able to leverage some sense of co-association. Here is where scale is both a challenge and a necessity. Given a single slice of wikipedia there are not enough interconnections visible, given a larger slice, it can be a time consuming task. To attempt to mitigate the scale, I did two things: one, migrate to a sqlitedb, in order to keep information on disk, and two implement multiprocessing over copies of that db, then unifying the db at the end. While the copies are strictly speaking unnecessary, I suspect that multiprocessors all accessing the same db is a recipe for disaster at worse, or futility, as the db will lock while being accessed. To accomplish this you will first need to run `build_records_db.py`, which takes your `page_embeddings.jsonl` file and writes that to a db. Then you can run either the single processor `build_l1_db_singleprocessor.py` or the multi-processor version `build_l1_multiprocessor.py`. After you run this your .db file (here named `wikilinksdata.db`) should have a records table and an l1 table. You now need to turn this into sparse embeddings. To do that you're going to use `build_record_embeddings_single.py`. 


### n.b. intersected sentences