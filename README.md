### Building knowledge vectors from Wiki dumps

The following code can be used to build knowledge vectors from wikidumps. 

In order to do this you will need to download a dump. This is an easy enough thing to source online. I used [this site (May, 2024)](https://wikimedia.bringyour.com/enwiki/20240501/), but there are many mirrors to choose from. A simple Google search should get you where you're going. From here there a bunch of choices of things to download. In the context of this I used the main pages as a single dump `enwiktionary-latest-pages-articles.xml`. However, some downloads represent slices of the whole, and others that represent aspects of pages--and this would certainly be worth considering in the future for a more surgical approach.  The main dump can also be easily parsed using `import lxml.etree as etree`. The advantage is that it doesn't have to be loaded all into memory at once. The disadvantage is that it is not as easily multiprocessed, and it isn't indexed. 

### Steps

* #### Identify page titles
    * This is the easiest and can be done a few ways. The simplest is to download `enwiki-20240501-all-titles`, which is a txt file of all the pages. Alternatively a single pass through the `enwiktionary-latest-pages-articles.xml` where you grab the title of every page will give you the same thing. In either case you might want to do some filtering. ***!CAREFUL!*** certain forms of preprocessing (such as .lower()) could have unintended consequences. Within the dump some pages are essentially redirect pages, so 'bill clinton' might in fact be a page, but it is not the same page as 'Bill Clinton', one is generally a redirect to another, and thus contains a link to the other, but nothing else.
    *  One form of filtering is to remove pages that have little or no links. To do this you can use `filter_wikipedia.py`, running this script will produce a new filtered dump which removed pages with 1 or fewer links. 
* #### Identify verbs of interest
    * I had previously scraped wiktionary and built a verb framework. Like all the other things, I could go back and do better, but it gave me a fairly large pool of verbs to use. This is in the data folder as `verb_tree`. For our purposes we are going to want to turn this into a straight list, for which you can used `flatten_verb_tree.py`. You can use that list to get verb counts from wikipedia pages using `wiki_verb_impact.py`. 
* #### Get links and link impact
    * You can then use `find_links.py`, which creates a mapping of titles to links (i.e. what are the links on a particular page), and then use `find_link_impact.py` in order to see which links are most impactful. With this we can do some filtering, note the filtering scripts such as: `filter_links.py`. 
* #### Map links and verbs to embeddings
    * From here we can build an embedding space using `build_semantic.py`. This creates a pair of embedding docs: `link_embeddings.json` and `verb_embeddings.json`, one for verbs and one for links. We want the pair because we can potentially use both embedding spaces (either in tandem or separately), and also because we are going to get the intersected links and verbs, to get phrases. 
* #### Main mining algorithm through Wikipages
    * Now we're ready to use the `mine_embeddings.py` code. This works with multiprocessing to speed up, and in that way it is relatively fast to process a full wiki dump. Here I have an example produced on a single slice of the wikidump. `enwiki-20240501-pages-articles15.xml-p17324603p17460152`. The result of the code should be two files: `intersected_sentences.jsonl` and `page_embeddings.jsonl`. It might be worth considering put it straight into a db, but since this is just an intermediate processing point it is perhaps most efficient to keep it in json or jsonl. At any point, depending on your computational resources you can run `jsonltojson.py` which can extract the verb embeddings, the link embeddings (or both) and save them to a .json file structure. Here you'll see see `linkout.json`. 
* #### Fit links to a 10k semantic space
    * Assuming you ran this on a full wiki dump you would then have every title with all of its page links. A full wikidump (as of this writing) is around 20 million pages. Some of those are vacuous, but even with those removed we're north of 15 million pages. If the goal is to get a page to know not only what it points to but also what is similar to it, we need to be able to leverage some sense of co-association. Here is where scale is both a challenge and a necessity. Given a single slice of wikipedia there are not enough interconnections visible, given a larger slice, it can be a time consuming task. To attempt to mitigate the scale, I did two things: one, migrate to a sqlitedb, in order to keep information on disk, and two implement multiprocessing over copies of that db, then unifying the db at the end. While the copies are strictly speaking unnecessary, I suspect that multiprocessors all accessing the same db is a recipe for disaster at worse, or futility, as the db will lock while being accessed. To accomplish this you will first need to run `build_records_db.py`, which takes your `page_embeddings.jsonl` file and writes that to a db. Then you can run either the single processor `build_l1_db_singleprocessor.py` or the multi-processor version `build_l1_multiprocessor.py`. After you run this your .db file (here named `wikilinksdata.db`) should have a records table and an l1 table (l1 being short for level1, I initially thought I would have to do this process repeatedly). 

    * What this l1 table represents are links that share a significant amount of identity with each other. What's interesting is that these links aren't symmetric, which is to say that some links are specific and others are more general. For example (and I'm making this up for illustrative purposes) "The Battle of Lexington and Concord" might share 80% similarity of its links with "historical", but obviously not vice-versa. This is actually to our advantage, as we can then count the instances in l1 using `count_l1.py`, and what floats to the surface are things like "historical", which I posit is exactly the kind of dimension we want. One could imagine a very clean and explainable semantic space being built up of 10k dimensions along these lines. These dimensions are less about word meaning, which is a form of knowledge abstracted of context, but rather knowledge meaning. We of course let the data and the computation do the heavy lifting, but for "The Battle of Lexington and Concord" we want it to be a decent chunk historical, and military, and American (supposing that was a dimension--if I were to make a knowledge space by hand I would have every nation), and some revolutionary, and battle, etc. Now whether or not these are in fact dimensions, I don't really care. And I suspect that way madness lies, but the supposition is that Wikipedia contains in itself (in its links, really) exactly that knowledge. And this is borne out by the use of the embeddings. 

    * After applying `count_l1.py`, we can then constrain our dimensional space to an arbitrary length. I chose 10k, but for future experiments, I think I will try a more constrained 2k space. Whether that will still capture sufficient dimensionality is an empirical question, but it would likely be more suitable for databased vector systems, and a database is likely essential for something of this scope. 

    * The next issue is to make sure that every possible link contributes to the semantic space, and not just the 10k links. We can do this by counting co-occurences across the corpus along those 10k elements. It's amazing how far one can get with simple counting. To do this, turn the 10k list into a 10k dictionary, then go through the corpus, and for every link associated with one of those 10k, it gets added as a subdict with a count value, the count value being incremented with every observation. In this way the link "historical" will only contribute (but in a maximal kind of way) to one dimension, but other terms, being co-observed with many of the 10k links might actually contribute (to a much smaller degree) to several dimensions at the same time. To do this we apply `populatel1_10k.py`, which we will then reverse using `reverse10kemb.py`. And at last we are ready to create our sparse embeddings with `build_sparse_embeddings_singleprocessor.py`. 
* #### Populating to pgvec or chromadb
    * Initially populated to chromadb using `build_chromadb.py`, but I found that this had memory issues, both in building, and in impelementation. I had to split this across multiple collections, which is impractical, especially because calling the chromadb had the same issue. However, it does make for fast and easily hostable server using `vecserver/app.py`. 
    * Building a pgvec database using this script `build_pgvedb.py` fixes the memory issues, but a 10k vec is not able to be indexed. I ran `pca_pgvecdb.py` to try and bring the dimensions down to 2k, but that code won't work as impelemented because batching makes PCA inconsistent. I'm inclined to build 2k dimensions from the ground up (see steps above).
* #### Querying the vectordb
    * To query the vector db you need to find the embedding of target, which is mapped in the db, and then do a similarity search based on that embedding. That logic is all wrapped up in `pgvecquery.ipynb`. I find that short of a server (as described above), a jupyter notebook is useful for experimenting. 

### Limitation of semantic space
    * The primary limitation of this semantic space is that it is possible to have a mismatch between user query, and what is available in the db. Of course, based on Wikipedia, there is an impressive amount of scale and range, but it would be valuable to allow other less common phrasing to be queryable. To dynamically build semantic embeddings that would be compatible with a db like this is outside of the scope, but intriguing. 
    * I think it's worth considering the relationship between an LLM, a db, and wikipedia itself. They all represent compressed forms of knowledge, and I would argue at face value that the LLM is in fact the most compressed, but with the possibility for the most fidelity. This would need to be borne out by investigation, but Llama7b is less than 20GB, whereas this DB is closer to 100GB. Of course, the means by which they yield an answer is very different, and it is possible that the most reliable solution is a hybrid implementation. 

### n.b. intersected sentences
    * The intersected sentences aren't emphasized in this repo, but they do represent an interesting extraction of knowledge statements from the wiki dump. The abstract.xml might pair nicely with this, and this could either be used in tandem with the tools above, wherein a user goes from a query, to similar links, to a pool of statements. 
