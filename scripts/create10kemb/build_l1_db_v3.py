import json
import sqlite3
from tqdm import tqdm
from multiprocessing import Pool, cpu_count
import os
import signal
import sys

def create_level1_table(cursor):
    """Create the level1 table in the SQLite database."""
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS level1alt (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            keys TEXT,
            name TEXT,
            names TEXT
        )
    ''')

def insert_records(cursor, records):
    """Insert multiple records into the SQLite database."""
    cursor.executemany('''
        INSERT INTO level1alt (name, keys, names)
        VALUES (?, ?, ?)
    ''', records)

def get_value_from_db(cursor, key, column="key"):
    """Retrieve the JSON-loaded value associated with a single key from the database."""
    cursor.execute(f'SELECT emb_idx, value FROM records WHERE {column}=?', (key,))
    row = cursor.fetchone()
    return (row[0], json.loads(row[1])) if row else (None, None)

def get_values_from_db(cursor, keys, column="key"):
    """Retrieve the JSON-loaded values associated with a list of keys from the database."""
    placeholders = ','.join(['?'] * len(keys))
    cursor.execute(f'SELECT key, emb_idx, value FROM records WHERE {column} IN ({placeholders})', keys)
    rows = cursor.fetchall()
    return [[row[0], {row[1]: json.loads(row[2])}] for row in rows]

def ensure_index(cursor, table, column):
    """Ensure that an index exists on the specified column in the table."""
    index_name = f"{table}_{column}_idx"
    cursor.execute(f"PRAGMA index_list({table})")
    indexes = cursor.fetchall()
    index_exists = any(index_name == index[1] for index in indexes)
    if not index_exists:
        cursor.execute(f"CREATE INDEX {index_name} ON {table} ({column})")
        print(f"Created index {index_name} on {table}({column})")

def find_coreferential_groups(original, name, keys, cursor):
    """Find the largest group of keys that are co-referential."""
    key_to_links = get_values_from_db(cursor, keys, "emb_idx")
    emb_idcs = {original}
    names = {name}
    candidates = set(keys)

    for elem in key_to_links:
        for k, v in elem[1].items():
            intersection = candidates.intersection(set(v))
            if len(intersection) >= 0.8 * len(v):
                emb_idcs.add(k)
                names.add(elem[0])
            else:
                pass
    return emb_idcs, names

def get_processed_names(write_db_filename):
    """Retrieve the names already processed and inserted into the database."""
    conn = sqlite3.connect(write_db_filename)
    cursor = conn.cursor()
    cursor.execute('SELECT name FROM level1alt')
    rows = cursor.fetchall()
    conn.close()
    return set(row[0] for row in rows)

def process_key_batch(read_db_filename, write_db_filename, keys, index):
    """Process a batch of keys to find and store co-referential groups."""
    read_conn = sqlite3.connect(read_db_filename)
    read_cursor = read_conn.cursor()
    write_conn = sqlite3.connect(write_db_filename)
    write_cursor = write_conn.cursor()
    create_level1_table(write_cursor)
    ensure_index(read_cursor, "records", "key")
    ensure_index(read_cursor, "records", "emb_idx")

    processed_names = get_processed_names(write_db_filename)

    print(f"Process {index} started with {len(keys)} keys.")

    batch_records = []
    for k in tqdm(keys, desc=f"Processing batch {index}"):
        if k in processed_names:
            continue
        original, links = get_value_from_db(read_cursor, k, "key")
        if original is None or links is None:
            continue
        coreferential_groups, used_names = find_coreferential_groups(original, k, links, read_cursor)
        batch_records.append((k, json.dumps(list(coreferential_groups)), json.dumps(list(used_names))))
        if len(batch_records) >= 100:  # Adjust batch size as needed
            insert_records(write_cursor, batch_records)
            write_conn.commit()
            batch_records.clear()

    if batch_records:
        insert_records(write_cursor, batch_records)
        write_conn.commit()

    read_conn.close()
    write_conn.close()
    print(f"Process {index} finished.")

def distribute_keys(keys, num_workers):
    """Distribute keys in a round-robin fashion across workers."""
    keys = list(keys)
    return [keys[i::num_workers] for i in range(num_workers)]

def cleanup(db_filenames):
    """Remove auxiliary files generated by SQLite."""
    for db_filename in db_filenames:
        for ext in ['-shm', '-wal']:
            file_to_remove = f"{db_filename}{ext}"
            if os.path.exists(file_to_remove):
                os.remove(file_to_remove)
                print(f"Removed file: {file_to_remove}")

def signal_handler(sig, frame):
    """Handle SIGINT (Ctrl+C) to clean up auxiliary files."""
    print("Interrupt received, cleaning up...")
    cleanup([f'{db_filename}_{i}.db' for i in range(num_workers)])
    sys.exit(0)

def main():
    global db_filename, num_workers
    db_filename = 'database/wikilinksdata'
    allkeys = 'link_impact.json'

    with open(allkeys, 'r') as f:
        keys = json.load(f)

    max_workers = 8  # Set a reasonable limit on the number of processes
    num_workers = min(max_workers, cpu_count())

    distributed_keys = distribute_keys(keys, num_workers)

    signal.signal(signal.SIGINT, signal_handler)

    with Pool(processes=num_workers) as pool:
        pool.starmap(process_key_batch, [(f'{db_filename}_{i}.db', f'{db_filename}_output_{i}.db', chunk, i) for i, chunk in enumerate(distributed_keys)])

    cleanup([f'{db_filename}_{i}.db' for i in range(num_workers)])

if __name__ == '__main__':
    main()
