import json
import sqlite3
from tqdm import tqdm
import os
import signal
import sys

def create_level1_table(cursor):
    """Create the level1 table in the SQLite database."""
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS level1alt (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            keys TEXT,
            name TEXT,
            names TEXT
        )
    ''')

def insert_records(cursor, records):
    """Insert multiple records into the SQLite database."""
    cursor.executemany('''
        INSERT INTO level1alt (name, keys, names)
        VALUES (?, ?, ?)
    ''', records)

def get_value_from_db(cursor, key, column="key"):
    """Retrieve the JSON-loaded value associated with a single key from the database."""
    cursor.execute(f'SELECT emb_idx, value FROM records WHERE {column}=?', (key,))
    row = cursor.fetchone()
    return (row[0], json.loads(row[1])) if row else (None, None)

def get_values_from_db(cursor, keys, column="key"):
    """Retrieve the JSON-loaded values associated with a list of keys from the database."""
    placeholders = ','.join(['?'] * len(keys))
    cursor.execute(f'SELECT key, emb_idx, value FROM records WHERE {column} IN ({placeholders})', keys)
    rows = cursor.fetchall()
    return [[row[0], {row[1]: json.loads(row[2])}] for row in rows]

def ensure_index(cursor, table, column):
    """Ensure that an index exists on the specified column in the table."""
    index_name = f"{table}_{column}_idx"
    cursor.execute(f"PRAGMA index_list({table})")
    indexes = cursor.fetchall()
    index_exists = any(index_name == index[1] for index in indexes)
    if not index_exists:
        cursor.execute(f"CREATE INDEX {index_name} ON {table} ({column})")
        print(f"Created index {index_name} on {table}({column})")

def find_coreferential_groups(original, name, keys, cursor):
    """Find the largest group of keys that are co-referential."""
    key_to_links = get_values_from_db(cursor, keys, "emb_idx")
    emb_idcs = {original}
    names = {name}
    candidates = set(keys)

    for elem in key_to_links:
        for k, v in elem[1].items():
            intersection = candidates.intersection(set(v))
            if len(intersection) >= 0.8 * len(v):
                emb_idcs.add(k)
                names.add(elem[0])
            else:
                pass
    return emb_idcs, names

def get_processed_names(write_db_filename):
    """Retrieve the names already processed and inserted into the database."""
    conn = sqlite3.connect(write_db_filename)
    cursor = conn.cursor()
    cursor.execute('SELECT name FROM level1alt')
    rows = cursor.fetchall()
    conn.close()
    return set(row[0] for row in rows)

def process_keys(read_db_filename, write_db_filename, keys):
    """Process keys to find and store co-referential groups."""
    read_conn = sqlite3.connect(read_db_filename)
    read_cursor = read_conn.cursor()
    write_conn = sqlite3.connect(write_db_filename)
    write_cursor = write_conn.cursor()
    create_level1_table(write_cursor)
    ensure_index(read_cursor, "records", "key")
    ensure_index(read_cursor, "records", "emb_idx")

    processed_names = get_processed_names(write_db_filename)

    batch_records = []
    for k in tqdm(keys, desc="Processing keys"):
        if k in processed_names:
            continue
        original, links = get_value_from_db(read_cursor, k, "key")
        if original is None or links is None:
            continue
        coreferential_groups, used_names = find_coreferential_groups(original, k, links, read_cursor)
        batch_records.append((k, json.dumps(list(coreferential_groups)), json.dumps(list(used_names))))
        if len(batch_records) >= 100:  # Adjust batch size as needed
            insert_records(write_cursor, batch_records)
            write_conn.commit()
            batch_records.clear()

    if batch_records:
        insert_records(write_cursor, batch_records)
        write_conn.commit()

    read_conn.close()
    write_conn.close()

def cleanup(db_filename):
    """Remove auxiliary files generated by SQLite."""
    for ext in ['-shm', '-wal']:
        file_to_remove = f"{db_filename}{ext}"
        if os.path.exists(file_to_remove):
            os.remove(file_to_remove)
            print(f"Removed file: {file_to_remove}")

def signal_handler(sig, frame):
    """Handle SIGINT (Ctrl+C) to clean up auxiliary files."""
    print("Interrupt received, cleaning up...")
    cleanup(db_filename)
    sys.exit(0)

def main():
    global db_filename
    db_filename = 'wikilinksdata.db'
    allkeys = 'data/link_impact_example.json'

    with open(allkeys, 'r') as f:
        keys = json.load(f)

    signal.signal(signal.SIGINT, signal_handler)

    process_keys(db_filename, db_filename, keys)

    cleanup(db_filename)

if __name__ == '__main__':
    main()
